{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero Algorithm - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was built to conduct experiences on the AlphaZero Algorithm and better understand its implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import games_mod # Games\n",
    "import policy_mod # neural network\n",
    "from play_mod import Play #functionalities of game\n",
    "import training_mod #neural network training\n",
    "from replay_buffer_dict import ReplayBuffer #centralized buffer\n",
    "from utils import DotDict #other utilities\n",
    "from log_data import LogData #logging class for monitoring purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game, Training, Competition, Benchmark and Play Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game settings\n",
    "game_settings = DotDict({\n",
    "    \"board_size\": (3,3),\n",
    "    \"N\": 3\n",
    "})\n",
    "\n",
    "# Self-play training settings\n",
    "game_training_settings = DotDict({\n",
    "    \"generations\": 100,\n",
    "    \"self_play_iterations\": 50,\n",
    "    \"data_augmentation_times\": 1\n",
    "})\n",
    "# alpha = 10 / average legal moves \n",
    "# https://medium.com/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5 \n",
    "\n",
    "# Self-play training settings\n",
    "mcts_settings = DotDict({\n",
    "    \"explore_steps\": 50,\n",
    "    \"temp\": 1.0,\n",
    "    \"dir_enabled\": True,\n",
    "    \"dir_eps\": 0.25,\n",
    "    \"dir_alpha\": 2.0,\n",
    "})\n",
    "\n",
    "# neural network settings\n",
    "nn_training_settings = DotDict({\n",
    "    \"load_policy\": False,\n",
    "    \"policy_path\": \"ai_ckp.pth\",\n",
    "    \"ckp_folder\":\"../ckp\",\n",
    "    \"lr\": .005, \n",
    "    \"weight_decay\": 1.e-4,\n",
    "    \"buffer_size_target\": 1000,\n",
    "    \"n_epochs\": 1,\n",
    "    \"batch_size\": 50\n",
    "})\n",
    "# set compet_freq at 0 for disabling the competition between current and trained network. \n",
    "# In this case the trained network replaces the current network at every generation\n",
    "\n",
    "benchmark_competition_settings = DotDict({\n",
    "    \"compet_freq\":0,\n",
    "    \"compet_rounds\": 2,\n",
    "    \"net_compet_threshold\": 0.0,\n",
    "    \"benchmark_freq\": 5,\n",
    "    \"benchmark_rounds\": 50,\n",
    "    \"mcts_iterations\": 1000,\n",
    "    \"mcts_random_moves\":0\n",
    "})\n",
    "\n",
    "# play settings\n",
    "play_settings = DotDict({\n",
    "    \"explore_steps\": 50,\n",
    "    \"temperature\": 0.01                         \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_competition_settings.compet_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = LogData()\n",
    "log_data.add_chart(\"nn_loss\", [\"nn_loss.csv\", ['iter', 'loss', 'value_loss', 'prob_loss']])\n",
    "log_data.add_chart(\"buffer\", [\"buffer.csv\", ['iter', 'wins', 'losses', 'draws']])\n",
    "log_data.add_chart(\"compet\", [\"compet.csv\",['iter', 'scores']])\n",
    "\n",
    "game=games_mod.ConnectN(game_settings)\n",
    "\n",
    "policy = policy_mod.Policy(nn_training_settings.policy_path, \n",
    "                           nn_training_settings, \n",
    "                           log_data)\n",
    "policy.save_weights()\n",
    "\n",
    "buffer = ReplayBuffer(nn_training_settings.buffer_size_target, \n",
    "                      nn_training_settings.batch_size, \n",
    "                      log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generations:   6%|▌         | 6/100 [01:46<33:43, 21.53s/it]Network replaced at generation 5\n",
      "Generations:   7%|▋         | 7/100 [01:54<26:14, 16.93s/it]Network replaced at generation 6\n",
      "Generations:   8%|▊         | 8/100 [02:02<21:29, 14.02s/it]Network replaced at generation 7\n",
      "Generations:   9%|▉         | 9/100 [02:09<18:14, 12.03s/it]Network replaced at generation 8\n",
      "Network replaced at generation 9\n",
      "Generations:  11%|█         | 11/100 [03:17<31:19, 21.12s/it]Network replaced at generation 10\n",
      "Generations:  12%|█▏        | 12/100 [03:26<25:22, 17.30s/it]Network replaced at generation 11\n",
      "Generations:  13%|█▎        | 13/100 [03:34<20:56, 14.44s/it]Network replaced at generation 12\n",
      "Generations:  14%|█▍        | 14/100 [03:42<17:49, 12.44s/it]Network replaced at generation 13\n",
      "Network replaced at generation 14\n",
      "Generations:  16%|█▌        | 16/100 [04:55<31:10, 22.27s/it]Network replaced at generation 15\n",
      "Generations:  17%|█▋        | 17/100 [05:03<24:57, 18.04s/it]Network replaced at generation 16\n",
      "Generations:  18%|█▊        | 18/100 [05:12<20:37, 15.10s/it]Network replaced at generation 17\n",
      "Generations:  19%|█▉        | 19/100 [05:20<17:27, 12.93s/it]Network replaced at generation 18\n",
      "Network replaced at generation 19\n",
      "Generations:  21%|██        | 21/100 [06:33<29:38, 22.51s/it]Network replaced at generation 20\n",
      "Generations:  22%|██▏       | 22/100 [06:41<23:33, 18.12s/it]Network replaced at generation 21\n",
      "Generations:  23%|██▎       | 23/100 [06:49<19:21, 15.09s/it]Network replaced at generation 22\n",
      "Generations:  24%|██▍       | 24/100 [06:57<16:10, 12.77s/it]Network replaced at generation 23\n",
      "Network replaced at generation 24\n",
      "Generations:  26%|██▌       | 26/100 [08:10<27:36, 22.38s/it]Network replaced at generation 25\n",
      "Generations:  27%|██▋       | 27/100 [08:18<21:53, 17.99s/it]Network replaced at generation 26\n",
      "Generations:  28%|██▊       | 28/100 [08:25<17:49, 14.86s/it]Network replaced at generation 27\n",
      "Generations:  29%|██▉       | 29/100 [08:34<15:12, 12.85s/it]Network replaced at generation 28\n",
      "Network replaced at generation 29\n",
      "Generations:  31%|███       | 31/100 [09:44<25:01, 21.77s/it]Network replaced at generation 30\n",
      "Generations:  32%|███▏      | 32/100 [09:52<19:48, 17.48s/it]Network replaced at generation 31\n",
      "Generations:  33%|███▎      | 33/100 [09:59<16:02, 14.36s/it]Network replaced at generation 32\n",
      "Generations:  34%|███▍      | 34/100 [10:06<13:31, 12.29s/it]Network replaced at generation 33\n",
      "Network replaced at generation 34\n",
      "Generations:  36%|███▌      | 36/100 [11:18<23:10, 21.72s/it]Network replaced at generation 35\n",
      "Generations:  37%|███▋      | 37/100 [11:26<18:27, 17.58s/it]Network replaced at generation 36\n",
      "Generations:  38%|███▊      | 38/100 [11:34<15:18, 14.82s/it]Network replaced at generation 37\n",
      "Generations:  39%|███▉      | 39/100 [11:42<13:07, 12.90s/it]Network replaced at generation 38\n",
      "Network replaced at generation 39\n",
      "Generations:  41%|████      | 41/100 [12:52<21:18, 21.68s/it]Network replaced at generation 40\n",
      "Generations:  42%|████▏     | 42/100 [13:00<17:03, 17.65s/it]Network replaced at generation 41\n",
      "Generations:  43%|████▎     | 43/100 [13:09<14:05, 14.84s/it]Network replaced at generation 42\n",
      "Generations:  44%|████▍     | 44/100 [13:18<12:16, 13.16s/it]Network replaced at generation 43\n",
      "Network replaced at generation 44\n",
      "Generations:  46%|████▌     | 46/100 [14:28<19:41, 21.87s/it]Network replaced at generation 45\n",
      "Generations:  47%|████▋     | 47/100 [14:36<15:33, 17.62s/it]Network replaced at generation 46\n",
      "Generations:  48%|████▊     | 48/100 [14:43<12:32, 14.48s/it]Network replaced at generation 47\n",
      "Generations:  49%|████▉     | 49/100 [14:50<10:30, 12.36s/it]Network replaced at generation 48\n",
      "Network replaced at generation 49\n",
      "Generations:  51%|█████     | 51/100 [16:03<17:57, 21.98s/it]Network replaced at generation 50\n",
      "Generations:  52%|█████▏    | 52/100 [16:11<14:12, 17.76s/it]Network replaced at generation 51\n",
      "Generations:  53%|█████▎    | 53/100 [16:19<11:35, 14.80s/it]Network replaced at generation 52\n",
      "Generations:  54%|█████▍    | 54/100 [16:26<09:43, 12.69s/it]Network replaced at generation 53\n",
      "Network replaced at generation 54\n",
      "Generations:  56%|█████▌    | 56/100 [17:40<16:24, 22.39s/it]Network replaced at generation 55\n",
      "Generations:  57%|█████▋    | 57/100 [17:49<13:11, 18.40s/it]Network replaced at generation 56\n",
      "Generations:  58%|█████▊    | 58/100 [17:57<10:47, 15.43s/it]Network replaced at generation 57\n",
      "Generations:  59%|█████▉    | 59/100 [18:06<09:06, 13.34s/it]Network replaced at generation 58\n",
      "Network replaced at generation 59\n",
      "Generations:  61%|██████    | 61/100 [19:23<15:17, 23.52s/it]Network replaced at generation 60\n",
      "Generations:  62%|██████▏   | 62/100 [19:31<11:58, 18.91s/it]Network replaced at generation 61\n",
      "Generations:  63%|██████▎   | 63/100 [19:39<09:34, 15.54s/it]Network replaced at generation 62\n",
      "Generations:  64%|██████▍   | 64/100 [19:47<08:01, 13.38s/it]Network replaced at generation 63\n",
      "Network replaced at generation 64\n",
      "Generations:  66%|██████▌   | 66/100 [21:02<13:01, 23.00s/it]Network replaced at generation 65\n",
      "Generations:  67%|██████▋   | 67/100 [21:10<10:06, 18.37s/it]Network replaced at generation 66\n",
      "Generations:  68%|██████▊   | 68/100 [21:17<08:05, 15.19s/it]Network replaced at generation 67\n",
      "Generations:  69%|██████▉   | 69/100 [21:25<06:43, 13.00s/it]Network replaced at generation 68\n",
      "Network replaced at generation 69\n",
      "Generations:  71%|███████   | 71/100 [22:37<10:42, 22.14s/it]Network replaced at generation 70\n",
      "Generations:  72%|███████▏  | 72/100 [22:45<08:15, 17.70s/it]Network replaced at generation 71\n",
      "Generations:  73%|███████▎  | 73/100 [22:53<06:45, 15.00s/it]Network replaced at generation 72\n",
      "Generations:  74%|███████▍  | 74/100 [23:02<05:41, 13.12s/it]Network replaced at generation 73\n",
      "Network replaced at generation 74\n",
      "Generations:  76%|███████▌  | 76/100 [24:13<08:48, 22.00s/it]Network replaced at generation 75\n",
      "Generations:  77%|███████▋  | 77/100 [24:20<06:42, 17.52s/it]Network replaced at generation 76\n",
      "Generations:  78%|███████▊  | 78/100 [24:28<05:19, 14.54s/it]Network replaced at generation 77\n",
      "Generations:  79%|███████▉  | 79/100 [24:36<06:32, 18.70s/it]Network replaced at generation 78\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5f40f14b6882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     log_data)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0malpha_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robots/machine_learning/rl/MyAlphaZero/src/training_mod.py\u001b[0m in \u001b[0;36mtraining_pipeline\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcts_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 )\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_exp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robots/machine_learning/rl/MyAlphaZero/src/training_mod.py\u001b[0m in \u001b[0;36mexecute_self_play\u001b[0;34m(game_settings, mcts_settings, policy)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mdir_eps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir_eps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mdir_alpha\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmcts_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 dirichlet_enabled= mcts_settings.dir_enabled)\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mcurrent_player\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmytree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmcts_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robots/machine_learning/rl/MyAlphaZero/src/mcts.py\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(self, orac_params, dir_eps, dir_alpha, dirichlet_enabled)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msibling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                         \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "alpha_0 = training_mod.AlphaZeroTraining(\n",
    "    game_settings, \n",
    "    game_training_settings,\n",
    "    mcts_settings,\n",
    "    nn_training_settings,\n",
    "    benchmark_competition_settings,\n",
    "    play_settings,\n",
    "    policy,\n",
    "    log_data)\n",
    "alpha_0.training_pipeline(buffer)\n",
    "t1 = time.time()\n",
    "print (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import policy_mod  # neural network\n",
    "\n",
    "def test_final_positions(game_state):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    frame = torch.tensor(game_state, dtype=torch.float, device=device).unsqueeze(0).unsqueeze(0)\n",
    "    policy_path = \"ai_ckp.pth\"\n",
    "    policy = policy_mod.Policy(policy_path, nn_training_settings)\n",
    "    policy.load_weights(policy_path)\n",
    "    print (frame)\n",
    "    v, p = policy.forward_batch(frame)\n",
    "    print(\"Probabilities = {}; Values = {}\".format(p, v))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-1.,  1., -1.],\n          [ 0.,  1.,  0.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 8.4970e-08,  8.4970e-08,  8.4970e-08,  1.2820e-03,  8.4970e-08,\n          1.3472e-03,  7.9750e-04,  9.9490e-01,  1.6758e-03]]); Values = tensor([[ 0.8834]])\n"
     ]
    }
   ],
   "source": [
    "game_state1 = np.array([[-1, 1, -1], [0, 1, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-1.,  1., -1.],\n          [ 0.,  1., -1.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 1.0181e-07,  1.0181e-07,  1.0181e-07,  1.5234e-04,  1.0181e-07,\n          1.0181e-07,  2.5640e-04,  9.9826e-01,  1.3306e-03]]); Values = tensor([[ 0.9417]])\n"
     ]
    }
   ],
   "source": [
    "game_state2 = np.array([[-1, 1, -1], [0, 1, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game with 2nd position not in the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 0.,  0.,  0.],\n          [ 0.,  0.,  0.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 0.1265,  0.0394,  0.1808,  0.0311,  0.3087,  0.0458,  0.0772,\n          0.0497,  0.1408]]); Values = tensor([[-0.1421]])\n"
     ]
    }
   ],
   "source": [
    "game_state3 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-1.,  0.,  0.],\n          [ 0.,  0.,  0.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 0.0011,  0.0643,  0.0552,  0.0646,  0.5669,  0.0362,  0.0459,\n          0.0493,  0.1165]]); Values = tensor([[-0.6335]])\n"
     ]
    }
   ],
   "source": [
    "game_state4 = np.array([[-1, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 1.,  0.,  0.],\n          [ 0.,  0., -1.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 0.0004,  0.1166,  0.4792,  0.0829,  0.1865,  0.0004,  0.1082,\n          0.0050,  0.0208]]); Values = tensor([[ 0.9842]])\n"
     ]
    }
   ],
   "source": [
    "game_state5 = np.array([[1, 0, 0], [0, 0, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-1.,  0., -1.],\n          [ 0.,  0.,  1.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 0.0002,  0.4954,  0.0002,  0.1477,  0.1413,  0.0002,  0.0997,\n          0.0775,  0.0378]]); Values = tensor([[-0.9286]])\n"
     ]
    }
   ],
   "source": [
    "game_state6 = np.array([[-1, 0, -1], [0, 0, 1], [0, 0, 0]])\n",
    "test_final_positions (game_state6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 1., -1.,  1.],\n          [ 0.,  0., -1.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 0.0000,  0.0000,  0.0000,  0.0092,  0.6929,  0.0000,  0.2636,\n          0.0016,  0.0327]]); Values = tensor([[ 0.9997]])\n"
     ]
    }
   ],
   "source": [
    "game_state7 = np.array([[1, -1, 1], [0, 0, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-1.,  1., -1.],\n          [ 0., -1.,  1.],\n          [ 0.,  0.,  0.]]]])\nProbabilities = tensor([[ 0.0001,  0.0001,  0.0001,  0.3398,  0.0001,  0.0001,  0.5126,\n          0.1091,  0.0381]]); Values = tensor([[-0.8974]])\n"
     ]
    }
   ],
   "source": [
    "game_state8 = np.array([[-1, 1, -1], [0, -1, 1], [0, 0, 0]])\n",
    "test_final_positions (game_state8)"
   ]
  },
  {
   "source": [
    "# Testing Symetries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def flip(x, dim):\n",
    "\n",
    "    indices = [slice(None)] * x.dim()\n",
    "    indices[dim] = torch.arange(\n",
    "        x.size(dim) - 1, -1, -1, dtype=torch.long, device=x.device\n",
    "    )\n",
    "    return x[tuple(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = lambda x: x\n",
    "t1 = lambda x: x[:, ::-1].copy()\n",
    "t2 = lambda x: x[::-1, :].copy()\n",
    "t3 = lambda x: x[::-1, ::-1].copy()\n",
    "t4 = lambda x: x.T\n",
    "# TO DO\n",
    "t5 = lambda x: x[:, ::-1].T.copy()\n",
    "t6 = lambda x: x[::-1, :].T.copy()\n",
    "t7 = lambda x: x[::-1, ::-1].T.copy()\n",
    "\n",
    "tlist = [t0, t1, t2, t3, t4, t7]\n",
    "tlist_half = [t0, t1, t2, t3]\n",
    "\n",
    "# inverse transformations\n",
    "t0inv = lambda x: x\n",
    "t1inv = lambda x: flip(x, 1)\n",
    "t2inv = lambda x: flip(x, 0)\n",
    "t3inv = lambda x: flip(flip(x, 0), 1)\n",
    "t4inv = lambda x: x.t()\n",
    "# TO DO\n",
    "t5inv = lambda x: flip(x, 1).t()\n",
    "t6inv = lambda x: flip(x, 0).t()\n",
    "t7inv = lambda x: flip(flip(x, 0), 1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, tinv = t7, t7inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_board = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "new_board = t(input_board)\n",
    "new_board_tensor = torch.tensor(input_board)\n",
    "prob = new_board_tensor.reshape(3, 3)\n",
    "old_board = tinv(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[9, 6, 3],\n",
       "        [8, 5, 2],\n",
       "        [7, 4, 1]]),\n",
       " tensor([[ 9,  6,  3],\n",
       "         [ 8,  5,  2],\n",
       "         [ 7,  4,  1]]))"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "new_board, old_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}