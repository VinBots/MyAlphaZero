{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero Algorithm - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was built to conduct experiences on the AlphaZero Algorithm and better understand its implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import games_mod # Games\n",
    "import policy_mod # neural network\n",
    "from play_mod import Play #functionalities of game\n",
    "import training_mod #neural network training\n",
    "from replay_buffer_dict import ReplayBuffer #centralized buffer\n",
    "from utils import DotDict #other utilities\n",
    "from log_data import LogData #logging class for monitoring purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game, Training, Competition, Benchmark and Play Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game settings\n",
    "game_settings = DotDict({\n",
    "    \"board_size\": (3,3),\n",
    "    \"N\": 3\n",
    "})\n",
    "\n",
    "# Self-play training settings\n",
    "game_training_settings = DotDict({\n",
    "    \"generations\": 100,\n",
    "    \"self_play_iterations\": 50,\n",
    "    \"explore_steps\": 50,\n",
    "    \"temp\": 1,\n",
    "    \"dir_eps\": 0.25,\n",
    "    \"dir_alpha\": 0.25,\n",
    "    \"data_augmentation_times\": 1\n",
    "})\n",
    "# alpha = 10 / average legal moves \n",
    "# https://medium.com/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5 \n",
    "\n",
    "# neural network settings\n",
    "nn_training_settings = DotDict({\n",
    "    \"load_policy\": False,\n",
    "    \"policy_path\": \"ai_ckp.pth\",\n",
    "    \"ckp_folder\":\"ckp\",\n",
    "    \"lr\": .005, \n",
    "    \"weight_decay\": 1.e-4,\n",
    "    \"buffer_size_target\": 1000,\n",
    "    \"n_epochs\": 1,\n",
    "    \"batch_size\": 50\n",
    "})\n",
    "\n",
    "benchmark_competition_settings = DotDict({\n",
    "    \"compet_freq\":0,\n",
    "    \"compet_rounds\": 10,\n",
    "    \"net_compet_threshold\": 0,\n",
    "    \"benchmark_freq\": 5,\n",
    "    \"benchmark_rounds\": 25,\n",
    "    \"mcts_iterations\": 1000,\n",
    "    \"mcts_random_moves\":0\n",
    "})\n",
    "\n",
    "# play settings\n",
    "play_settings = DotDict({\n",
    "    \"explore_steps\": 50,\n",
    "    \"temperature\": 0.01                         \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = LogData()\n",
    "log_data.add_chart(\"nn_loss\", [\"nn_loss.csv\", ['iter', 'loss', 'value_loss', 'prob_loss']])\n",
    "log_data.add_chart(\"buffer\", [\"buffer.csv\", ['iter', 'wins', 'losses', 'draws']])\n",
    "log_data.add_chart(\"compet\", [\"compet.csv\",['iter', 'scores']])\n",
    "\n",
    "game=games_mod.ConnectN(game_settings)\n",
    "\n",
    "policy = policy_mod.Policy(nn_training_settings.policy_path, \n",
    "                           nn_training_settings, \n",
    "                           log_data)\n",
    "policy.save_weights()\n",
    "\n",
    "buffer = ReplayBuffer(nn_training_settings.buffer_size_target, \n",
    "                      nn_training_settings.batch_size, \n",
    "                      log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4105.611351013184\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "alpha_0 = training_mod.AlphaZeroTraining(\n",
    "    game_settings, \n",
    "    game_training_settings,\n",
    "    nn_training_settings,\n",
    "    benchmark_competition_settings,\n",
    "    play_settings,\n",
    "    policy,\n",
    "    log_data)\n",
    "alpha_0.training_pipeline(buffer)\n",
    "t1 = time.time()\n",
    "print (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import policy_mod  # neural network\n",
    "\n",
    "def test_final_positions(game_state):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    frame = torch.tensor(game_state, dtype=torch.float, device=device).unsqueeze(0).unsqueeze(0)\n",
    "    policy_path = \"ckp/ai_ckp.pth\"\n",
    "    policy = policy_mod.Policy(policy_path)\n",
    "    policy.load_weights(policy_path)\n",
    "    print (frame)\n",
    "    v, p = policy.forward_batch(frame)\n",
    "    print(\"Probabilities = {}; Values = {}\".format(p, v))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  1., -1.],\n",
      "          [ 0.,  1.,  0.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 2.8055e-11,  2.8055e-11,  2.8055e-11,  2.9608e-05,  2.8055e-11,\n",
      "          5.0483e-04,  1.0367e-05,  9.9900e-01,  4.5571e-04]]); Values = tensor([[ 0.9988]])\n"
     ]
    }
   ],
   "source": [
    "game_state1 = np.array([[-1, 1, -1], [0, 1, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  1., -1.],\n",
      "          [ 0.,  1., -1.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 2.7364e-10,  2.7364e-10,  2.7364e-10,  3.4019e-05,  2.7364e-10,\n",
      "          2.7364e-10,  1.0967e-04,  9.9851e-01,  1.3413e-03]]); Values = tensor([[ 0.9975]])\n"
     ]
    }
   ],
   "source": [
    "game_state2 = np.array([[-1, 1, -1], [0, 1, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game with 2nd position not in the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 0.0995,  0.0325,  0.1270,  0.0348,  0.1866,  0.0445,  0.2864,\n",
      "          0.0515,  0.1372]]); Values = tensor([[ 0.2140]])\n"
     ]
    }
   ],
   "source": [
    "game_state3 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 0.0006,  0.0778,  0.0271,  0.0911,  0.5970,  0.0731,  0.0451,\n",
      "          0.0466,  0.0417]]); Values = tensor([[-0.9739]])\n"
     ]
    }
   ],
   "source": [
    "game_state4 = np.array([[-1, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  0.,  0.],\n",
      "          [ 0.,  0., -1.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 0.0002,  0.0883,  0.5215,  0.0187,  0.0370,  0.0002,  0.3095,\n",
      "          0.0018,  0.0226]]); Values = tensor([[ 0.9971]])\n"
     ]
    }
   ],
   "source": [
    "game_state5 = np.array([[1, 0, 0], [0, 0, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  0., -1.],\n",
      "          [ 0.,  0.,  1.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 0.0002,  0.3189,  0.0002,  0.0737,  0.4490,  0.0002,  0.0321,\n",
      "          0.1056,  0.0200]]); Values = tensor([[-0.9912]])\n"
     ]
    }
   ],
   "source": [
    "game_state6 = np.array([[-1, 0, -1], [0, 0, 1], [0, 0, 0]])\n",
    "test_final_positions (game_state6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1., -1.,  1.],\n",
      "          [ 0.,  0., -1.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 0.0001,  0.0001,  0.0001,  0.0229,  0.4994,  0.0001,  0.3820,\n",
      "          0.0158,  0.0797]]); Values = tensor([[ 0.9998]])\n"
     ]
    }
   ],
   "source": [
    "game_state7 = np.array([[1, -1, 1], [0, 0, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  1., -1.],\n",
      "          [ 0., -1.,  1.],\n",
      "          [ 0.,  0.,  0.]]]])\n",
      "Probabilities = tensor([[ 0.0004,  0.0004,  0.0004,  0.0930,  0.0004,  0.0004,  0.4493,\n",
      "          0.2257,  0.2300]]); Values = tensor([[-0.9852]])\n"
     ]
    }
   ],
   "source": [
    "game_state8 = np.array([[-1, 1, -1], [0, -1, 1], [0, 0, 0]])\n",
    "test_final_positions (game_state8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
