{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero Algorithm - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was built to conduct experiences on the AlphaZero Algorithm and better understand its implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import games_mod # Games\n",
    "import policy_mod # neural network\n",
    "from play_mod import Play #functionalities of game\n",
    "import training_mod #neural network training\n",
    "from replay_buffer_dict import ReplayBuffer #centralized buffer\n",
    "from utils import DotDict #other utilities\n",
    "from log_data import LogData #logging class for monitoring purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game, Training, Competition, Benchmark and Play Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game settings\n",
    "game_settings = DotDict({\n",
    "    \"board_size\": (3,3),\n",
    "    \"N\": 3,\n",
    "    \"discount_enabled\": False\n",
    "})\n",
    "\n",
    "# Self-play training settings\n",
    "game_training_settings = DotDict({\n",
    "    \"generations\": 100,\n",
    "    \"self_play_iterations\": 50,\n",
    "    \"data_augmentation_times\": 1\n",
    "})\n",
    "# alpha = 10 / average legal moves \n",
    "# https://medium.com/oracledevs/lessons-from-alphazero-part-3-parameter-tweaking-4dceb78ed1e5 \n",
    "\n",
    "# Self-play training settings\n",
    "mcts_settings = DotDict({\n",
    "    \"explore_steps\": 50,\n",
    "    \"temp\": 1.0,\n",
    "    \"dir_enabled\": True,\n",
    "    \"dir_eps\": 0.25,\n",
    "    \"dir_alpha\": 2.0,\n",
    "})\n",
    "\n",
    "# neural network settings\n",
    "nn_training_settings = DotDict({\n",
    "    \"load_policy\": False,\n",
    "    \"policy_path\": \"ai_ckp.pth\",\n",
    "    \"ckp_folder\":\"../ckp\",\n",
    "    \"lr\": .005, \n",
    "    \"weight_decay\": 1.e-4,\n",
    "    \"buffer_size_target\": 1000,\n",
    "    \"n_epochs\": 1,\n",
    "    \"batch_size\": 50\n",
    "})\n",
    "# set compet_freq at 0 for disabling the competition between current and trained network. \n",
    "# In this case the trained network replaces the current network at every generation\n",
    "\n",
    "benchmark_competition_settings = DotDict({\n",
    "    \"compet_freq\":0,\n",
    "    \"compet_rounds\": 2,\n",
    "    \"net_compet_threshold\": 0.0,\n",
    "    \"benchmark_freq\": 5,\n",
    "    \"benchmark_rounds\": 50,\n",
    "    \"mcts_iterations\": 1000,\n",
    "    \"mcts_random_moves\":0\n",
    "})\n",
    "\n",
    "# play settings\n",
    "play_settings = DotDict({\n",
    "    \"explore_steps\": 50,\n",
    "    \"temperature\": 0.01                         \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_competition_settings.compet_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = LogData()\n",
    "log_data.add_chart(\"nn_loss\", [\"nn_loss.csv\", ['iter', 'loss', 'value_loss', 'prob_loss']])\n",
    "log_data.add_chart(\"buffer\", [\"buffer.csv\", ['iter', 'wins', 'losses', 'draws']])\n",
    "log_data.add_chart(\"compet\", [\"compet.csv\",['iter', 'scores']])\n",
    "\n",
    "game=games_mod.ConnectN(game_settings)\n",
    "\n",
    "policy = policy_mod.Policy(nn_training_settings.policy_path, \n",
    "                           nn_training_settings, \n",
    "                           log_data)\n",
    "policy.save_weights()\n",
    "\n",
    "buffer = ReplayBuffer(nn_training_settings.buffer_size_target, \n",
    "                      nn_training_settings.batch_size, \n",
    "                      log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_to_buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5f40f14b6882>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     log_data)\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0malpha_0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vince\\Documents\\AI\\MyAlphaZero\\src\\training_mod.py\u001b[0m in \u001b[0;36mtraining_pipeline\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             call_back_func = partial(\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0madd_to_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                 \u001b[0mbuffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdata_augmentation_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_augmentation_times\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_to_buffer' is not defined"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "alpha_0 = training_mod.AlphaZeroTraining(\n",
    "    game_settings, \n",
    "    game_training_settings,\n",
    "    mcts_settings,\n",
    "    nn_training_settings,\n",
    "    benchmark_competition_settings,\n",
    "    play_settings,\n",
    "    policy,\n",
    "    log_data)\n",
    "alpha_0.training_pipeline(buffer)\n",
    "t1 = time.time()\n",
    "print (t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import policy_mod  # neural network\n",
    "\n",
    "def test_final_positions(game_state):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    frame = torch.tensor(game_state, dtype=torch.float, device=device).unsqueeze(0).unsqueeze(0)\n",
    "    policy_path = \"ai_ckp.pth\"\n",
    "    policy = policy_mod.Policy(policy_path, nn_training_settings)\n",
    "    policy.load_weights(policy_path)\n",
    "    print (frame)\n",
    "    v, p = policy.forward_batch(frame)\n",
    "    print(\"Probabilities = {}; Values = {}\".format(p, v))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state1 = np.array([[-1, 1, -1], [0, 1, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "game_state2 = np.array([[-1, 1, -1], [0, 1, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game with 2nd position not in the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_state3 = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state4 = np.array([[-1, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "test_final_positions (game_state4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state5 = np.array([[1, 0, 0], [0, 0, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state6 = np.array([[-1, 0, -1], [0, 0, 1], [0, 0, 0]])\n",
    "test_final_positions (game_state6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state7 = np.array([[1, -1, 1], [0, 0, -1], [0, 0, 0]])\n",
    "test_final_positions (game_state7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state8 = np.array([[-1, 1, -1], [0, -1, 1], [0, 0, 0]])\n",
    "test_final_positions (game_state8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Symetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def flip(x, dim):\n",
    "\n",
    "    indices = [slice(None)] * x.dim()\n",
    "    indices[dim] = torch.arange(\n",
    "        x.size(dim) - 1, -1, -1, dtype=torch.long, device=x.device\n",
    "    )\n",
    "    return x[tuple(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = lambda x: x\n",
    "t1 = lambda x: x[:, ::-1].copy()\n",
    "t2 = lambda x: x[::-1, :].copy()\n",
    "t3 = lambda x: x[::-1, ::-1].copy()\n",
    "t4 = lambda x: x.T\n",
    "# TO DO\n",
    "t5 = lambda x: x[:, ::-1].T.copy()\n",
    "t6 = lambda x: x[::-1, :].T.copy()\n",
    "t7 = lambda x: x[::-1, ::-1].T.copy()\n",
    "\n",
    "tlist = [t0, t1, t2, t3, t4, t7]\n",
    "tlist_half = [t0, t1, t2, t3]\n",
    "\n",
    "# inverse transformations\n",
    "t0inv = lambda x: x\n",
    "t1inv = lambda x: flip(x, 1)\n",
    "t2inv = lambda x: flip(x, 0)\n",
    "t3inv = lambda x: flip(flip(x, 0), 1)\n",
    "t4inv = lambda x: x.t()\n",
    "# TO DO\n",
    "t5inv = lambda x: flip(x, 1).t()\n",
    "t6inv = lambda x: flip(x, 0).t()\n",
    "t7inv = lambda x: flip(flip(x, 0), 1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, tinv = t7, t7inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_board = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "new_board = t(input_board)\n",
    "new_board_tensor = torch.tensor(input_board)\n",
    "prob = new_board_tensor.reshape(3, 3)\n",
    "old_board = tinv(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_board, old_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "main.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [4, 2, 3]\n",
    "arr2 = [4, 2, 3]\n",
    "arr3 = [1, 2, 3]\n",
    "arr4 = [3, 2, 3]\n",
    "p1 = 1\n",
    "p2 = 2\n",
    "p3 = 3\n",
    "p4 = 4\n",
    "list_arr = [[arr1, p1], [arr2,p2], [arr3,p3], [arr4,p4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "\n",
    "for arr, p in list_arr:\n",
    "    if arr i new_list:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(a).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
